{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "feature_extraction.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyN4tk2oKSTQ/WHg8a/lXqjB",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AbdullahMakhdoom/COVID-19_scrape/blob/master/feature_extraction.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vhZqlpI3tAWU"
      },
      "source": [
        "Caltech 101 dataset has a size of 131 MB with approximately 9000 images in 101 categories (about 40 to 800 images per category). \n",
        "\n",
        "*Note* : *A 102nd category called \"BACKGROUND_Google\" consisting of random images is also present in this dataset,which needs to be deleted.*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "apLnAJlMsl-d"
      },
      "source": [
        "Download the Caltech101 dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eKZ0vAYiwUDj"
      },
      "source": [
        "!pip install gdown"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zsRVi4JjscoO"
      },
      "source": [
        "!gdown https://drive.google.com/uc?id=137RyRjvTBkBiIfeYBNZBtViDHQ6_Ewsp --output caltech101.tar.gz\n",
        "!tar -xvzf caltech101.tar.gz\n",
        "!mkdir datasets\n",
        "!mv 101_ObjectCategories datasets/caltech101\n",
        "!rm -rf datasets/caltech101/BACKGROUND_Google"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kqaUWbz1f2Op"
      },
      "source": [
        "**Import Necessary Packages**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "scrolled": false,
        "id": "w3-s1COTuFxV"
      },
      "source": [
        "import numpy as np\n",
        "from numpy.linalg import norm\n",
        "import pickle\n",
        "from tqdm import tqdm, notebook\n",
        "import os\n",
        "import random\n",
        "import time\n",
        "import math\n",
        "import tensorflow\n",
        "from tensorflow.keras.preprocessing import image\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.applications.resnet50 import ResNet50, preprocess_input\n",
        "from tensorflow.keras.applications.vgg16 import VGG16\n",
        "from tensorflow.keras.applications.vgg19 import VGG19\n",
        "from tensorflow.keras.applications.mobilenet import MobileNet\n",
        "from tensorflow.keras.applications.inception_v3 import InceptionV3\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, Flatten, Dense, Dropout, GlobalAveragePooling2D\n"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nQaxAnijvbhs"
      },
      "source": [
        "A Utility function that allows us to choose a pretrained model with all the necessary details for experimentation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hbo23jhtvaW0"
      },
      "source": [
        "def model_picker(name):\n",
        "    if (name == 'vgg16'):\n",
        "        model = VGG16(weights='imagenet',\n",
        "                      include_top=False,\n",
        "                      input_shape=(224, 224, 3),\n",
        "                      pooling='max')\n",
        "    elif (name == 'vgg19'):\n",
        "        model = VGG19(weights='imagenet',\n",
        "                      include_top=False,\n",
        "                      input_shape=(224, 224, 3),\n",
        "                      pooling='max')\n",
        "    elif (name == 'mobilenet'):\n",
        "        model = MobileNet(weights='imagenet',\n",
        "                          include_top=False,\n",
        "                          input_shape=(224, 224, 3),\n",
        "                          pooling='max',\n",
        "                          depth_multiplier=1,\n",
        "                          alpha=1)\n",
        "    elif (name == 'inception'):\n",
        "        model = InceptionV3(weights='imagenet',\n",
        "                            include_top=False,\n",
        "                            input_shape=(224, 224, 3),\n",
        "                            pooling='max')\n",
        "    elif (name == 'resnet'):\n",
        "        model = ResNet50(weights='imagenet',\n",
        "                         include_top=False,\n",
        "                         input_shape=(224, 224, 3),\n",
        "                        pooling='max')\n",
        "    elif (name == 'xception'):\n",
        "        model = Xception(weights='imagenet',\n",
        "                         include_top=False,\n",
        "                         input_shape=(224, 224, 3),\n",
        "                         pooling='max')\n",
        "    else:\n",
        "        print(\"Specified model not available\")\n",
        "    return model"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Iep7Pkz7vvP3"
      },
      "source": [
        "Picking 'ResNet50' as our model architecture."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7ohIWXDlvvFi",
        "outputId": "ca861461-deb1-4a20-8cd4-7b95d4c54ccd"
      },
      "source": [
        "model_architecture = 'resnet'\n",
        "model = model_picker(model_architecture)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/resnet/resnet50_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "94773248/94765736 [==============================] - 1s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9SqFUe7_wOas"
      },
      "source": [
        "Defining a function to extract image features from an image and a model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mjZ2V9G8wO6A"
      },
      "source": [
        "def extract_features(img_path, model):\n",
        "    input_shape = (224, 224, 3)\n",
        "    img = image.load_img(img_path,\n",
        "                         target_size=(input_shape[0], input_shape[1]))\n",
        "    img_array = image.img_to_array(img)\n",
        "    expanded_img_array = np.expand_dims(img_array, axis=0)\n",
        "    preprocessed_img = preprocess_input(expanded_img_array)\n",
        "    features = model.predict(preprocessed_img)\n",
        "    flattened_features = features.flatten()\n",
        "    normalized_features = flattened_features / norm(flattened_features)\n",
        "    return normalized_features"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yVw3K9ghwa2r"
      },
      "source": [
        "Defining a helper function to recursively get all image files under a root directory.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d8J9zpMPwYWG"
      },
      "source": [
        "extensions = ['.jpg', '.JPG', '.jpeg', '.JPEG', '.png', '.PNG']\n",
        "\n",
        "def get_file_list(root_dir):\n",
        "    file_list = []\n",
        "    for root, directories, filenames in os.walk(root_dir):\n",
        "        for filename in filenames:\n",
        "            if any(ext in filename for ext in extensions):\n",
        "                file_list.append(os.path.join(root, filename))\n",
        "    return file_list"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P6iSo6mLw6FR"
      },
      "source": [
        "Running the extraction over the entire Caltech101 dataset and timing it using tqdm package."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qIRs-BHJw21o"
      },
      "source": [
        "# path to the your datasets\n",
        "root_dir = 'datasets/caltech101'\n",
        "filenames = sorted(get_file_list(root_dir))\n",
        "\n",
        "feature_list = []\n",
        "for i in notebook.tqdm(range(len(filenames))):\n",
        "    feature_list.append(extract_features(filenames[i], model))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tDLNVzh_OCxB"
      },
      "source": [
        "# make folder named 'data'\n",
        "!mkdir data"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BB1JT89HjsLq"
      },
      "source": [
        "Saving the features and filenames for later use."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OW_Aeno8jLLk"
      },
      "source": [
        "Utilizing parallel processing of GPUs will speed up the feature extraction step.\n",
        "(Keeping the batch_size = 64)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oIfBXQFqOAXP",
        "outputId": "53e12e04-496a-46fb-e727-228925957a1e"
      },
      "source": [
        "batch_size = 64\n",
        "datagen = tensorflow.keras.preprocessing.image.ImageDataGenerator(preprocessing_function=preprocess_input)\n",
        "\n",
        "generator = datagen.flow_from_directory(root_dir,\n",
        "                                        target_size=(224, 224),\n",
        "                                        batch_size=batch_size,\n",
        "                                        class_mode=None,\n",
        "                                        shuffle=False)\n",
        "\n",
        "num_images = len(generator.filenames)\n",
        "num_epochs = int(math.ceil(num_images / batch_size))\n",
        "\n",
        "start_time = time.time()\n",
        "feature_list = []\n",
        "feature_list = model.predict_generator(generator, num_epochs)\n",
        "end_time = time.time()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 8677 images belonging to 101 classes.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training.py:1905: UserWarning: `Model.predict_generator` is deprecated and will be removed in a future version. Please use `Model.predict`, which supports generators.\n",
            "  warnings.warn('`Model.predict_generator` is deprecated and '\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m-JdGr8rO9Tl"
      },
      "source": [
        "for i, features in enumerate(feature_list):\n",
        "    feature_list[i] = features / norm(features)\n",
        "\n",
        "feature_list = feature_list.reshape(num_images, -1)\n",
        "\n",
        "print(\"Num images   = \", len(generator.classes))\n",
        "print(\"Shape of feature_list = \", feature_list.shape)\n",
        "print(\"Time taken in sec = \", end_time - start_time)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m2UmLo_KgPUi"
      },
      "source": [
        "Utilizing Colab's GPU fastened the feature extraction process approximately 3 times (4080(sec)/ 1331(sec) = 3.065)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fvY_wyGMxCFF"
      },
      "source": [
        "pickle.dump(feature_list, open('data/features-caltech101-resnet.pickle', 'wb'))\n",
        "pickle.dump(filenames, open('data/filenames-caltech101.pickle', 'wb'))"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AD_iMAwAhd8X"
      },
      "source": [
        "**Fine-tuned Model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "Bs7ey7R9uFxn"
      },
      "source": [
        "TRAIN_SAMPLES = 8677\n",
        "NUM_CLASSES = 101\n",
        "IMG_WIDTH, IMG_HEIGHT = 224, 224"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ep5PfIFoUdKz"
      },
      "source": [
        "train_datagen = ImageDataGenerator(preprocessing_functino = preprocess_input,\n",
        "                                     rotation_range=20,\n",
        "                                     width_shift_range=0.2,\n",
        "                                     height_shift_range=0.2,\n",
        "                                     zoom_range=0.2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        },
        "id": "BcgjGI6MiLX7",
        "outputId": "bb8e4335-7387-4b33-a2a8-44ee50399216"
      },
      "source": [
        "train_generator = train_datagen.flow_from_directory(root_dir,\n",
        "                                                    target_size=(IMG_WIDTH,\n",
        "                                                                 IMG_HEIGHT),\n",
        "                                                    batch_size = 64,\n",
        "                                                    shuffle = True,\n",
        "                                                    seed = 42,\n",
        "                                                    class_mode= 'categorical')\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-efa6ee22e1e5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m train_generator = train_datagen.flow_from_directory(root_dir,\n\u001b[0m\u001b[1;32m      2\u001b[0m                                                     target_size=(IMG_WIDTH,\n\u001b[1;32m      3\u001b[0m                                                                  IMG_HEIGHT),\n\u001b[1;32m      4\u001b[0m                                                     \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m                                                     \u001b[0mshuffle\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'train_datagen' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "39McpnrMiLVM"
      },
      "source": [
        "def model_maker():\n",
        "  base_model = ResNet50(include_top = False,\n",
        "                        input_shape=(IMG_WIDTH, IMG_HEIGHT, 3))\n",
        "  for layer in base_model.layers[:]:\n",
        "    layer.trainable = False\n",
        "  input = Input(shape = (IMG_WIDTH, IMG_HEIGHT, 3))\n",
        "  custom_model = base_model(input)\n",
        "  custom_model = GlobalAveragePooling2D()(custom_model)\n",
        "  custom_model = Dense(64, activation='relu')(custom_model)\n",
        "  custom_model = Dropout(0.5)(custom_model)\n",
        "  predictions = Dense(NUM_CLASSES, activation='softmax')(custom_model)\n",
        "  return Model(inputs=input, outputs=prediction)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Za1PJgtIiLTB"
      },
      "source": [
        "model_finetuned = model_maker()\n",
        "model_finetuned.compile(loss = 'categorical_crossentropy',\n",
        "                        optimizer=tensorflow.keras.optimizers.Adam(0.001),\n",
        "                        metrics=['acc'])\n",
        "model_finetuned.fit_generator(\n",
        "    train_generator,\n",
        "    steps_per_epoch = math.ceil(float(TRAIN_SAMPLES) / batch_size),\n",
        "    epochs = 10)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4qBNhTueneIi"
      },
      "source": [
        "model_finetuned.save('./data/model-finetuned.h5')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jWuUYnd4neFc"
      },
      "source": [
        "start_time = time.time()\n",
        "feature_list_finetuned = []\n",
        "feature_list_finetuned = model_finetuned.predict_generator(generator, num_epochs)\n",
        "end_time = time.time()\n",
        "\n",
        "for i, features_finetuned in enumerate(feature_list_finetuned):\n",
        "    feature_list_finetuned[i] = features_finetuned / norm(features_finetuned)\n",
        "\n",
        "feature_list = feature_list_finetuned.reshape(num_images, -1)\n",
        "\n",
        "print(\"Num images   = \", len(generator.classes))\n",
        "print(\"Shape of feature_list = \", feature_list.shape)\n",
        "print(\"Time taken in sec = \", end_time - start_time)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pfec21p5neCN"
      },
      "source": [
        "pickle.dump(\n",
        "    feature_list,\n",
        "    open('./data/features-caltech101-resnet-finetuned.pickle', 'wb'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aB06zLd8iK_V"
      },
      "source": [
        ""
      ]
    }
  ]
}